{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "\n",
    "import xlsxwriter\n",
    "from itertools import chain\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "from SharePoint import *\n",
    "from hlpr import *\n",
    "from static import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latest_search_files(folder_path):\n",
    "    path = glob.glob(os.path.join(folder_path, '*'))\n",
    "    dict_ = dict()\n",
    "\n",
    "    for p in path:\n",
    "        fullname = os.path.split(p)[-1].split('.')[0]\n",
    "        cutoff = fullname.lower().find('nat')\n",
    "        k = fullname[:cutoff-1]\n",
    "\n",
    "        if k in dict_.keys():\n",
    "            dict_[k].append(p)\n",
    "        else:  \n",
    "            dict_[k] = []\n",
    "            dict_[k].append(p)\n",
    "\n",
    "    latest_file_path = [max(dict_[k], key=os.path.getctime) for k in dict_.keys()]\n",
    "    return latest_file_path\n",
    "\n",
    "\n",
    "def download_flatfile(app, media):\n",
    "    # dynamically create media dicts?????\n",
    "    media_dicts = {'social':{'fb':None,'li':None,'pi':None,'tw':None},\n",
    "                   'search':None,\n",
    "                   'digital':None}    \n",
    "        \n",
    "#     app = SharePoint('AmericanExpressUSGABM')\n",
    "    app.list_contents(SHAREPOINT_FLAT_PATH)\n",
    "        \n",
    "    # download flatfiles\n",
    "    for f in app.file_paths:\n",
    "        app.download_files(DATA_PATH)\n",
    "        \n",
    "    # store results in dictionary\n",
    "    if media=='social':\n",
    "        for p in media_dicts[media].keys():    \n",
    "            path = glob.glob(os.path.join(DATA_PATH, f\"{p}*\"))\n",
    "            if len(path)>0:\n",
    "                media_dicts[media][p] = pd.read_csv(path[0])\n",
    "\n",
    "    else:\n",
    "        path = glob.glob(os.path.join(DATA_PATH, f\"{media}*\"))\n",
    "        media_dicts[media] = pd.read_csv(path[0])\n",
    "\n",
    "    return media_dicts        \n",
    "\n",
    "def last_archive_date(app):\n",
    "    '''\n",
    "    Get the date of the current flat file.\n",
    "    '''\n",
    "    archive_dict = dict()\n",
    "    for f in app.file_paths:\n",
    "        try:\n",
    "            archive_date = os.path.split(f)[-1].split('_')[-1].split('.')[0]\n",
    "            archive_dict[int(archive_date)] = archive_date\n",
    "        except:\n",
    "            continue\n",
    "    last_archive_date = archive_dict[max(archive_dict, key=int)]\n",
    "    return last_archive_date\n",
    "\n",
    "def download_archives(app, last_archive_date_=0):\n",
    "    # get list of data folders and files\n",
    "    app.list_contents(SHAREPOINT_DATA_PATH)\n",
    "    for f in app.folder_paths:\n",
    "        archive_date = os.path.split(f)[-1]\n",
    "        try:    \n",
    "            if int(archive_date) > int(last_archive_date_):\n",
    "                app.folder_paths, app.file_paths = [], []\n",
    "                app.list_contents(f)\n",
    "                if len(app.file_paths)>0:\n",
    "                    data_subpath = os.path.join(DATA_PATH, archive_date)\n",
    "                    os.makedirs(data_subpath, exist_ok=True)\n",
    "                    app.download_files(data_subpath)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    len_ = len(os.listdir(DATA_PATH))\n",
    "    if len_==0:\n",
    "        print('FlatFile is up-to-date')\n",
    "    else:\n",
    "        return True\n",
    "        \n",
    "def create_flatfile(media, redshift_metrics_filename, last_flatfile=None):\n",
    "    '''\n",
    "    Create flat files.\n",
    "    '''\n",
    "    print(f\"\\nCreating {media} flat file\")\n",
    "    if media=='social':\n",
    "        social_flatfile(redshift_metrics_filename, last_flatfile)\n",
    "    elif media=='search':\n",
    "        search_flatfile(redshift_metrics_filename, last_flatfile)\n",
    "        \n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def social_flatfile(redshift_metrics_filename, last_flatfile=None):\n",
    "    '''\n",
    "    Create social flat files.\n",
    "    '''\n",
    "    col_order = ['archive_date', 'week', 'date', 'platform', 'campaign_name', 'campaign_id', 'adset', 'adset_id', 'variable', 'value']\n",
    "    \n",
    "    # params\n",
    "    pth = glob.glob(os.path.join(ASSETS_PATH, f\"*{redshift_metrics_filename}*\"))[0]\n",
    "    df_metrics = pd.read_excel(pth, sheet_name='metric_map')\n",
    "    \n",
    "    # store contents\n",
    "    platform_dict = {'fb': [], 'li':[], 'pi':[], 'tw':[]}\n",
    "    \n",
    "    # loop 1: load data to dictionary\n",
    "    for g in glob.glob(os.path.join(DATA_PATH, '*')):\n",
    "        for p in platform_dict.keys():\n",
    "            path = glob.glob(os.path.join(g, f\"{p}*\"))\n",
    "            if len(path)>0:\n",
    "                latest_file_path = max(path, key=os.path.getctime)\n",
    "                archive_date = int(os.path.split(os.path.split(latest_file_path)[0])[-1])\n",
    "                results = import_social(p, [latest_file_path], df_metrics) # using hlpr function\n",
    "                df_pre = results['pre_data']\n",
    "                df_pre['archive_date'] = archive_date\n",
    "\n",
    "                platform_dict[p].append(df_pre)\n",
    "\n",
    "    # given the platform, combine all data\n",
    "    platform_dict = {k:pd.concat(platform_dict[k], sort=False) for k in platform_dict.keys() if len(platform_dict[k])>0}\n",
    "    \n",
    "    time.sleep(2) # intermission\n",
    "    \n",
    "    # loop 2: create flat file with new data\n",
    "    for p in platform_dict.keys():\n",
    "        try:\n",
    "            # group dimensions\n",
    "            df = platform_dict[p].copy()\n",
    "            group_cols = [c for c in df.columns if c.lower()!='value']\n",
    "            df_grp = df.groupby(group_cols)['value'].sum().to_frame().reset_index()\n",
    "\n",
    "            # get latest based on id and archive date\n",
    "            flatfile_group_cols = media_args[media]['flatfile']['group_columns'][p]\n",
    "            max_date_map = df_grp.groupby(flatfile_group_cols)['archive_date'].max().to_frame().reset_index()\n",
    "            max_archive_date = max(max_date_map['archive_date'])\n",
    "            flatfile = max_date_map.merge(df_grp, how='left')\n",
    "            \n",
    "            # rename date field\n",
    "            date_field = social_param_fields[p]['date']\n",
    "            flatfile.rename(columns={date_field:'date'}, inplace=True)\n",
    "\n",
    "            # export\n",
    "            filename = f\"{p}_flatfile_{max_archive_date}.csv\"\n",
    "            \n",
    "            if not(last_flatfile is None):\n",
    "                # add previous flatfile\n",
    "                flatfile_pre = last_flatfile['social'][p].copy()\n",
    "                flatfile = pd.concat([flatfile_pre, flatfile], sort=False)\n",
    "                flatfile.drop_duplicates(inplace=True)\n",
    "\n",
    "            flatfile = flatfile[col_order]\n",
    "            flatfile.to_csv(os.path.join(OUTPUTS_PATH, filename), index=False)\n",
    "        \n",
    "        except TypeError as e:\n",
    "            print(f\"        Skipping {p} due to TypeError: {e}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "def search_flatfile(redshift_metrics_filename, last_flatfile=None):\n",
    "    '''\n",
    "    Create search flatfile.\n",
    "    '''\n",
    "    pth = glob.glob(os.path.join(ASSETS_PATH, f\"*{redshift_metrics_filename}*\"))[0]\n",
    "    df_metrics = pd.read_excel(pth, sheet_name='sa360_metrics')\n",
    "\n",
    "    list_of_dfs = []\n",
    "    for g in glob.glob(os.path.join(DATA_PATH, '*')): # loop in folders\n",
    "        try:\n",
    "            archive_date = int(os.path.split(g)[-1])\n",
    "        except ValueError:\n",
    "            continue\n",
    "        path = latest_search_files(g)\n",
    "        for p in path:\n",
    "            df = pd.read_excel(p)\n",
    "            id_vars = list(df.columns[:11])\n",
    "            df_trans = df.melt(id_vars=id_vars)\n",
    "            df_trans['archive_date'] = archive_date\n",
    "            list_of_dfs.append(df_trans)\n",
    "\n",
    "    flatfile_pre = pd.concat(list_of_dfs).reset_index(drop=True)\n",
    "    flatfile_pre['From'] = pd.to_datetime(flatfile_pre['From'])\n",
    "\n",
    "    max_date_map = flatfile_pre.groupby(FLATFILE_GROUP_COLS)['archive_date'].max().to_frame().reset_index()\n",
    "    max_archive_date = max(max_date_map['archive_date'])\n",
    "    flatfile = max_date_map.merge(flatfile_pre, how='left')\n",
    "    \n",
    "    if not(last_flatfile is None):\n",
    "        # add previous flatfile\n",
    "        flatfile_pre = last_flatfile['search'].copy()\n",
    "        flatfile = pd.concat([flatfile_pre, flatfile], sort=False)\n",
    "        flatfile.drop_duplicates(inplace=True)\n",
    "        \n",
    "    filename = f\"{media}_flatfile_{max_archive_date}.csv\"\n",
    "    flatfile.to_csv(os.path.join(OUTPUTS_PATH, filename), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "media = 'search'\n",
    "\n",
    "SHAREPOINT_DATA_PATH = media_args[media]['sharepoint']['data']\n",
    "SHAREPOINT_MAPPING_PATH = media_args[media]['sharepoint']['mapping']\n",
    "SHAREPOINT_QA_PATH = media_args[media]['sharepoint']['qa']\n",
    "SHAREPOINT_FLAT_PATH = media_args[media]['sharepoint']['flat']\n",
    "REDSHIFT_METRICS_QUERY = media_args[media]['redshift']['all_metrics']\n",
    "REDSHIFT_METRICS_FILENAME = media_args[media]['redshift']['metric_filename']\n",
    "REDSHIFT_METRICS_SHEETNAME = media_args[media]['redshift']['metric_sheetname']\n",
    "REDSHIFT_QA_QUERY = media_args[media]['redshift']['qa_qry']\n",
    "FLATFILE_GROUP_COLS = media_args[media]['flatfile']['group_columns']\n",
    "FLATFILE_FINAL_FILENAME = media_args[media]['flatfile']['final_filename']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Connect to SharePoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to SharePoint: https://interpublic.sharepoint.com/sites/AmericanExpressUSGABM\n"
     ]
    }
   ],
   "source": [
    "app = SharePoint('AmericanExpressUSGABM')\n",
    "# app.list_contents(SHAREPOINT_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating search flat file\n",
      "\n",
      "Deleting contents in data folder\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Before loading from the archive, we need to check if 04FlatFiles is empty.\n",
    "If 04FlatFiles is empty, go to the data folder to download all archive files locally.\n",
    "'''\n",
    "app.list_contents(SHAREPOINT_FLAT_PATH) # list folders and files\n",
    "\n",
    "if len(app.file_paths)==0:\n",
    "    download_archives(app)\n",
    "\n",
    "    create_flatfile(media, REDSHIFT_METRICS_FILENAME)\n",
    "#     # some function to load flat files to SharePoint\n",
    "    \n",
    "else:\n",
    "    last_archive_date_ = last_archive_date(app)\n",
    "    if download_archives(app, last_archive_date_):\n",
    "        last_flatfile = download_flatfile(app, media) # dictionary of flat files in 04flatfile\n",
    "        create_flatfile(media, REDSHIFT_METRICS_FILENAME, last_flatfile)\n",
    "    \n",
    "delete_local_data(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last_flatfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
