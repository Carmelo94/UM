{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "\n",
    "# import win32com.client as win32\n",
    "# from pathlib import Path\n",
    "import xlsxwriter\n",
    "from itertools import chain\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "from SharePoint import *\n",
    "from hlpr import *\n",
    "from static import *\n",
    "from format_qa import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# win32c = win32.constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "media = 'search'\n",
    "\n",
    "SHAREPOINT_DATA_PATH = media_args[media]['sharepoint']['data']\n",
    "SHAREPOINT_MAPPING_PATH = media_args[media]['sharepoint']['mapping']\n",
    "SHAREPOINT_QA_PATH = media_args[media]['sharepoint']['qa']\n",
    "SHAREPOINT_FLAT_PATH = media_args[media]['sharepoint']['flat']\n",
    "REDSHIFT_METRICS_QUERY = media_args[media]['redshift']['all_metrics']\n",
    "REDSHIFT_METRICS_FILENAME = media_args[media]['redshift']['metric_filename']\n",
    "REDSHIFT_METRICS_SHEETNAME = media_args[media]['redshift']['metric_sheetname']\n",
    "REDSHIFT_QA_QUERY = media_args[media]['redshift']['qa_qry']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Connect to SharePoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connected to: https://interpublic.sharepoint.com/sites/AmericanExpressUSGABM\n",
      "\n",
      "listing files from: Measurement%20%20Analytics%20Folder/GABM/Global%20Analytics/DigitalQA/Search/01Sandbox/SA360_Files/Gmail/National\n",
      "\n",
      "downloading data to: C:\\Users\\carmelo.urena\\Documents\\Main\\OneDrive_BAK\\Amex\\DigitalQA\\SharePoint\\sharepoint_api\\data\n",
      "\n",
      "archiving data in SharePoint\n",
      "\n",
      "listing files from: Measurement%20%20Analytics%20Folder/GABM/Global%20Analytics/DigitalQA/Search/02Mapping\n",
      "\n",
      "downloading data to: C:\\Users\\carmelo.urena\\Documents\\Main\\OneDrive_BAK\\Amex\\DigitalQA\\SharePoint\\sharepoint_api\\assets\n",
      "\n"
     ]
    }
   ],
   "source": [
    "app = SharePoint('AmericanExpressUSGABM')\n",
    "\n",
    "# download data\n",
    "app.list_contents(SHAREPOINT_DATA_PATH)\n",
    "app.download_files(DATA_PATH)\n",
    "app.archive_files()\n",
    "\n",
    "# download floodlight mapping\n",
    "app.list_contents(SHAREPOINT_MAPPING_PATH)\n",
    "app.download_files(ASSETS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Floodlight Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import metrics\n",
    "pth = glob.glob(os.path.join(ASSETS_PATH, f\"*{REDSHIFT_METRICS_FILENAME}*\"))[0]\n",
    "df_metrics = pd.read_excel(pth, sheet_name='FloodLight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import SA360 Data** <br>\n",
    "* Import data that was downloaded from the BAE SharePoint site <br>\n",
    "* Get list of floodlights by their custom floodlight names, as they appear in SA360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sa360 data\n",
    "list_of_floodlights = []\n",
    "list_of_dfs = []\n",
    "\n",
    "files = glob.glob(os.path.join(DATA_PATH, '*.xlsx'))\n",
    "for f in files:\n",
    "    df = pd.read_excel(f)\n",
    "    \n",
    "    # column check and add floodlights\n",
    "    if list(df.columns[:17]) == sa360_col_check:\n",
    "        list_of_floodlights.append(list(df.columns[17:]))\n",
    "    else:\n",
    "        print(f'\\ncolumn name mismatch: {f}\\n')\n",
    "        \n",
    "    # transpose sa360 data\n",
    "    id_vars = list(df.columns[:11])\n",
    "    df_trans = df.melt(id_vars = id_vars)\n",
    "    list_of_dfs.append(df_trans)\n",
    "\n",
    "# combine sa360 data\n",
    "df_ui = pd.concat(list_of_dfs)\n",
    "df_ui = df_ui[-df_ui['variable'].isin(['CTR', 'Avg CPC', 'Avg pos'])].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get Parameters for DB** <br>\n",
    "* advertiser <br>\n",
    "* start date <br>\n",
    "* end date <br>\n",
    "* metrics <br>\n",
    "     * floodlights_columns: custom SA360 floodlight names <br>\n",
    "     * floodlights_names: real floodlight names from the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of advertisers/accounts for database pull\n",
    "db_advertisers = \"','\".join(list(df_ui['Account'].unique()))\n",
    "\n",
    "# get dates\n",
    "db_start_date = datetime.strftime(df_ui['From'].min(), '%Y-%m-%d')\n",
    "db_end_date = datetime.strftime(df_ui['From'].max() + timedelta(days=6), '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust floodlights\n",
    "floodlights_columns = list(chain(*list_of_floodlights)) # unpack list\n",
    "floodlights_columns = list(dict.fromkeys(floodlights_columns,None).keys()) # remove dupes\n",
    "\n",
    "# filter metrics\n",
    "floodlights_names = df_metrics[df_metrics['sa360_col_name'].isin(floodlights_columns)]\n",
    "db_metrics = ['clicks', 'impr', 'cost']\n",
    "\n",
    "if len(floodlights_names['sa360_col_name'].unique()) == len(floodlights_columns):\n",
    "    # metrics for database\n",
    "#     db_metrics = list(floodlights_names['metric']) + db_metrics\n",
    "    db_metrics = \"','\".join(list(floodlights_names['metric']) + db_metrics)\n",
    "else:\n",
    "    db_metrics = \"','\".join(db_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Query Database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data from redshift\n",
    "qry_txt = get_qry_text(REDSHIFT_QA_QUERY)\n",
    "qry_txt = qry_txt.replace('load_metrics', db_metrics) \\\n",
    "                 .replace('load_advertisers', db_advertisers) \\\n",
    "                 .replace('load_start_date', db_start_date) \\\n",
    "                 .replace('load_end_date', db_end_date)\n",
    "\n",
    "# print(f\"Query Parameters\\n{'-'*50}\")\n",
    "# print(f\"advertisers: {db_advertisers}\\n\\nmetrics: {db_metrics}\\n\\nstart_date: {db_start_date}\\nend_date: {db_end_date}\")\n",
    "\n",
    "time.sleep(1)\n",
    "df_qry_raw = run_qry(qry_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map floodlights\n",
    "df_qry = df_qry_raw.copy()\n",
    "df_qry = df_qry.merge(floodlights_names, how='left', on=['advertiser', 'metric'])\n",
    "df_qry['sa360_col_name'] = df_qry.apply(lambda x: x['metric'].title() if x['metric'] in ['impr', 'cost', 'clicks'] else x['sa360_col_name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# null results\n",
    "df_qry_na = df_qry[df_qry['sa360_col_name'].isna()]\n",
    "df_qry_na_metrics = df_qry_na[['advertiser', 'metric']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "if len(df_qry_na_metrics.merge(floodlights_names, how='inner')) > 0:\n",
    "    print('advertiser and metric combination is not in df_qry, check sa360_metrics.xlsx')\n",
    "    print(df_qry_na_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qry = df_qry[-df_qry['sa360_col_name'].isna()] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine DB and UI Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# align ui column names\n",
    "ui_col_rename = {\n",
    "    'From': 'week',\n",
    "    'Account': 'advertiser',\n",
    "    'Campaign': 'campaign_name',\n",
    "    'variable': 'sa360_col_name',\n",
    "    'value': 'value'\n",
    "}\n",
    "\n",
    "df_ui_fltr = df_ui[ui_col_rename.keys()]\n",
    "df_ui_fltr.rename(columns=ui_col_rename, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qry_fltr = df_qry[ui_col_rename.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add source column\n",
    "df_qry_fltr['source'], df_ui_fltr['source'] = 'redshift', 'sa360' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qa = pd.concat([df_qry_fltr, df_ui_fltr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qa['advertiser'] = df_qa['advertiser'].str.replace('Amex - Shop small', 'Amex - Shop Small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qa['week'] = pd.to_datetime(df_qa['week'])\n",
    "df_qa['week'] = df_qa['week'].apply(lambda x: x - timedelta(days=x.weekday()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QA Pivots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvts_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_params_dict = {'last_updated':datetime.now(), 'advertisers': db_advertisers, 'start_date': db_start_date, 'end_date': db_end_date, 'sql_qry': qry_txt}\n",
    "details = pd.DataFrame.from_dict(db_params_dict, orient='index').reset_index()\n",
    "details.columns = ['variable', 'value']\n",
    "\n",
    "pvts_dict['details'] = details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create qa views\n",
    "#====================\n",
    "# raw data view\n",
    "df_v0 = df_qa.copy()\n",
    "\n",
    "df_v0 = df_v0.pivot_table(index=['advertiser', 'campaign_name', 'week', 'sa360_col_name'], columns=['source'], values='value', aggfunc='sum').reset_index().fillna(0)\n",
    "df_v0 = df_v0.melt(id_vars=['advertiser', 'campaign_name', 'week', 'sa360_col_name'])\n",
    "df_v0 = df_v0.sort_values(by=['advertiser', 'campaign_name', 'week', 'sa360_col_name']).reset_index(drop=True)\n",
    "df_v0.rename(columns={'sa360_col_name':'metric'}, inplace=True)\n",
    "\n",
    "# redshift v sa360\n",
    "df_v1 = df_qa.copy()\n",
    "\n",
    "df_v1 = df_v1.pivot_table(index=['advertiser', 'campaign_name', 'week', 'sa360_col_name'], columns=['source'], values='value', aggfunc='sum').reset_index()\n",
    "df_v1['%_diff'] = abs((df_v1['redshift']/df_v1['sa360'])-1)\n",
    "df_v1.rename(columns={'sa360_col_name':'metric'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_args = {\n",
    "    'advertiser': {'index':['advertiser'],\n",
    "                   'dim_cutoff': 1},\n",
    "        \n",
    "    'campaign': {'index':['advertiser', 'campaign_name'], \n",
    "                 'dim_cutoff': 2},\n",
    "    \n",
    "    'week': {'index':['advertiser', 'campaign_name', 'week'],\n",
    "             'dim_cutoff': 3}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pvts_dict = dict()\n",
    "\n",
    "for k in view_args.keys():\n",
    "    df_pvt = df_qa_week = df_qa.pivot_table(index=view_args[k]['index'], columns=['sa360_col_name', 'source'], values='value', aggfunc='sum').reset_index()\n",
    "    \n",
    "    list_of_metric_diff = []\n",
    "    metrics = df_pvt.columns.levels[0][:list(df_pvt.columns.levels[0]).index(view_args[k]['index'][-1])]\n",
    "    for m in metrics:\n",
    "        df_temp = df_pvt.copy()\n",
    "        df_temp = df_temp[m]\n",
    "        df_temp.columns = [f\"{m}_{c}\" for c in df_temp.columns]\n",
    "        \n",
    "        # fill na based on condition \n",
    "        col0 = df_temp.columns[0]\n",
    "        col1 = df_temp.columns[1]\n",
    "        df_temp[col0] = df_temp.apply(lambda x: 0 if np.isnan(x[col0]) and not(np.isnan(x[col1])) else x[col0], axis=1)\n",
    "        df_temp[col1] = df_temp.apply(lambda x: 0 if np.isnan(x[col1]) and not(np.isnan(x[col0])) else x[col1], axis=1)\n",
    "        \n",
    "        df_temp[f\"{m}_%_diff\"] = (df_temp.iloc[:,1]/df_temp.iloc[:,0])-1\n",
    "        df_temp[f\"{m}_%_diff\"] = df_temp[f\"{m}_%_diff\"].apply(lambda x: 1 if x == float('inf') else x)\n",
    "        \n",
    "        list_of_metric_diff.append(df_temp)\n",
    "        \n",
    "#         df_temp[f\"{m}_%_diff\"] = df_temp[f\"{m}_%_diff\"].apply(lambda x: 1 if x == float('inf') else x)\n",
    "\n",
    "    metric_diffs = pd.concat(list_of_metric_diff, axis=1)\n",
    "    \n",
    "    df_pvt_qa = pd.concat([df_pvt.iloc[:, :view_args[k]['dim_cutoff']], metric_diffs], axis=1)\n",
    "    df_pvt_qa.columns = [c[0] if type(c) is tuple else c for c in df_pvt_qa.columns]\n",
    "    \n",
    "    pvts_dict[k] = df_pvt_qa\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvts_dict['raw_data'], pvts_dict['redshift_v_sa360'] = df_v0, df_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear out no value campaigns\n",
    "campaign_qa = pvts_dict['campaign'].copy()\n",
    "remove_campaigns = list(campaign_qa[campaign_qa.iloc[:, 2:].sum(axis=1) == 0.0]['campaign_name'].unique())\n",
    "\n",
    "# clean dictionary\n",
    "keys = list(pvts_dict.keys())\n",
    "for k in keys[2:]:\n",
    "    pvts_dict[k] = pvts_dict[k][-(pvts_dict[k]['campaign_name'].isin(remove_campaigns))]\n",
    "    pvts_dict[k].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Export**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_filename = f\"QA_Search_{app.dt}.xlsx\"\n",
    "writer = pd.ExcelWriter(os.path.join(OUTPUTS_PATH, qa_filename), engine='xlsxwriter')\n",
    "\n",
    "for k in pvts_dict.keys():\n",
    "    pvts_dict[k].to_excel(writer, sheet_name=f\"{k}\", index=False)\n",
    "        \n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_qa(media, os.path.join(OUTPUTS_PATH, qa_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Upload to SharePoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_outputs_path = max(glob.glob(os.path.join(OUTPUTS_PATH, 'QA*')), key=os.path.getmtime)\n",
    "app.upload_files(SHAREPOINT_QA_PATH, local_outputs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa():\n",
    "# initiate SharePoint\n",
    "# ========================================================================================================================\n",
    "    app = SharePoint('AmericanExpressUSGABM')\n",
    "\n",
    "    # download data\n",
    "    app.list_contents(SHAREPOINT_DATA_PATH)\n",
    "    app.download_files(DATA_PATH)\n",
    "    app.archive_files()\n",
    "\n",
    "    # download floodlight mapping\n",
    "    app.list_contents(SHAREPOINT_MAPPING_PATH)\n",
    "    app.download_files(ASSETS_PATH)\n",
    "\n",
    "# qa\n",
    "# ========================================================================================================================\n",
    "    if media == 'search':\n",
    "        qa_resutls = search_qa() # return dictionary\n",
    "\n",
    "# save qa file\n",
    "# ========================================================================================================================\n",
    "    qa_filename = f\"QA_{media.title()}_{app.dt}.xlsx\"\n",
    "    writer = pd.ExcelWriter(os.path.join(OUTPUTS_PATH, qa_filename), engine='xlsxwriter')\n",
    "\n",
    "    for k in qa_resutls.keys():\n",
    "        qa_resutls[k].to_excel(writer, sheet_name=f\"{k}\", index=False)\n",
    "    writer.save()\n",
    "\n",
    "# format qa file\n",
    "# ========================================================================================================================\n",
    "    format_qa(media, os.path.join(OUTPUTS_PATH, qa_filename))\n",
    "\n",
    "# upload to SharePoint\n",
    "# add script to run multiple times on error (max 3)\n",
    "# ========================================================================================================================\n",
    "    local_outputs_path = max(glob.glob(os.path.join(OUTPUTS_PATH, 'QA*')), key=os.path.getmtime)\n",
    "    app.upload_files(SHAREPOINT_QA_PATH, local_outputs_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_qa(redshift_metrics_filename, redshift_qa_query):\n",
    "# import metrics\n",
    "# ========================================================================================================================\n",
    "    pth = glob.glob(os.path.join(ASSETS_PATH, f\"*{redshift_metrics_filename}*\"))[0]\n",
    "    df_metrics = pd.read_excel(pth, sheet_name='FloodLight')\n",
    "\n",
    "# import sa360 data\n",
    "# add to error log for metric mismatches\n",
    "# ========================================================================================================================\n",
    "    list_of_floodlights = []\n",
    "    list_of_dfs = []\n",
    "\n",
    "    files = glob.glob(os.path.join(DATA_PATH, '*.xlsx'))\n",
    "    for f in files:\n",
    "        df = pd.read_excel(f)\n",
    "\n",
    "        # column check and add floodlights\n",
    "        if list(df.columns[:17]) == sa360_col_check:\n",
    "            list_of_floodlights.append(list(df.columns[17:]))\n",
    "        else:\n",
    "            print(f'\\ncolumn name mismatch: {f}\\n')\n",
    "\n",
    "        # transpose sa360 data\n",
    "        id_vars = list(df.columns[:11])\n",
    "        df_trans = df.melt(id_vars = id_vars)\n",
    "        list_of_dfs.append(df_trans)\n",
    "\n",
    "# combine sa360 data\n",
    "# ========================================================================================================================\n",
    "    df_ui = pd.concat(list_of_dfs)\n",
    "    df_ui = df_ui[-df_ui['variable'].isin(['CTR', 'Avg CPC', 'Avg pos'])].reset_index(drop=True)\n",
    "\n",
    "# create database aruments\n",
    "# add to error log if floodlight mismatch\n",
    "# ========================================================================================================================\n",
    "    db_advertisers = \"','\".join(list(df_ui['Account'].unique()))\n",
    "    db_start_date = datetime.strftime(df_ui['From'].min(), '%Y-%m-%d')\n",
    "    db_end_date = datetime.strftime(df_ui['From'].max() + timedelta(days=6), '%Y-%m-%d')\n",
    "\n",
    "    # adjust floodlights\n",
    "    floodlights_columns = list(chain(*list_of_floodlights)) # unpack list\n",
    "    floodlights_columns = list(dict.fromkeys(floodlights_columns,None).keys()) # remove dupes\n",
    "\n",
    "    # filter metrics\n",
    "    floodlights_names = df_metrics[df_metrics['sa360_col_name'].isin(floodlights_columns)]\n",
    "    db_metrics = ['clicks', 'impr', 'cost']\n",
    "\n",
    "    if len(floodlights_names['sa360_col_name'].unique()) == len(floodlights_columns):\n",
    "        db_metrics = \"','\".join(list(floodlights_names['metric']) + db_metrics)\n",
    "    else:\n",
    "        db_metrics = \"','\".join(db_metrics)\n",
    "\n",
    "# get data from redshift\n",
    "# ========================================================================================================================\n",
    "    qry_txt = get_qry_text(redshift_qa_query)\n",
    "    qry_txt = qry_txt.replace('load_metrics', db_metrics) \\\n",
    "                     .replace('load_advertisers', db_advertisers) \\\n",
    "                     .replace('load_start_date', db_start_date) \\\n",
    "                     .replace('load_end_date', db_end_date)\n",
    "\n",
    "    time.sleep(1)\n",
    "    df_qry_raw = run_qry(qry_txt)\n",
    "\n",
    "# map floodlights and remove nulls\n",
    "# add nulls to error log\n",
    "# ========================================================================================================================\n",
    "    df_qry = df_qry_raw.copy()\n",
    "    df_qry = df_qry.merge(floodlights_names, how='left', on=['advertiser', 'metric'])\n",
    "    df_qry['sa360_col_name'] = df_qry.apply(lambda x: x['metric'].title() if x['metric'] in ['impr', 'cost', 'clicks'] else x['sa360_col_name'], axis=1)\n",
    "\n",
    "    # null results\n",
    "    df_qry_na = df_qry[df_qry['sa360_col_name'].isna()]\n",
    "    df_qry_na_metrics = df_qry_na[['advertiser', 'metric']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    if len(df_qry_na_metrics.merge(floodlights_names, how='inner')) > 0:\n",
    "        print('advertiser and metric combination is not in df_qry, check sa360_metrics.xlsx')\n",
    "        print(df_qry_na_metrics)\n",
    "\n",
    "# set up the qa dataframe\n",
    "# ========================================================================================================================\n",
    "    df_qry = df_qry[-df_qry['sa360_col_name'].isna()]\n",
    "\n",
    "    # align ui column names\n",
    "    ui_col_rename = {\n",
    "        'From': 'week',\n",
    "        'Account': 'advertiser',\n",
    "        'Campaign': 'campaign_name',\n",
    "        'variable': 'sa360_col_name',\n",
    "        'value': 'value'\n",
    "    }\n",
    "\n",
    "    # filter ui and database results columns\n",
    "    df_ui_fltr = df_ui[ui_col_rename.keys()]\n",
    "    df_ui_fltr.rename(columns=ui_col_rename, inplace=True)\n",
    "    df_qry_fltr = df_qry[ui_col_rename.values()]\n",
    "\n",
    "    # add source column\n",
    "    df_qry_fltr['source'], df_ui_fltr['source'] = 'redshift', 'sa360'\n",
    "    df_qa = pd.concat([df_qry_fltr, df_ui_fltr])\n",
    "    df_qa['advertiser'] = df_qa['advertiser'].str.replace('Amex - Shop small', 'Amex - Shop Small')\n",
    "\n",
    "# set up the dictionary file to save results\n",
    "# ========================================================================================================================\n",
    "    pvts_dict = dict()\n",
    "\n",
    "    # details\n",
    "    db_params_dict = {'last_updated':datetime.now(), 'advertisers': db_advertisers, 'start_date': db_start_date, 'end_date': db_end_date, 'sql_qry': qry_txt}\n",
    "    details = pd.DataFrame.from_dict(db_params_dict, orient='index').reset_index()\n",
    "    details.columns = ['variable', 'value']\n",
    "    pvts_dict['details'] = details\n",
    "\n",
    "    # create qa views\n",
    "    df_v0 = df_qa.copy()\n",
    "    df_v0 = df_v0.pivot_table(index=['advertiser', 'campaign_name', 'week', 'sa360_col_name'], columns=['source'], values='value', aggfunc='sum').reset_index().fillna(0)\n",
    "    df_v0 = df_v0.melt(id_vars=['advertiser', 'campaign_name', 'week', 'sa360_col_name'])\n",
    "    df_v0 = df_v0.sort_values(by=['advertiser', 'campaign_name', 'week', 'sa360_col_name']).reset_index(drop=True)\n",
    "    df_v0.rename(columns={'sa360_col_name':'metric'}, inplace=True)\n",
    "\n",
    "    # redshift v sa360\n",
    "    df_v1 = df_qa.copy()\n",
    "\n",
    "    df_v1 = df_v1.pivot_table(index=['advertiser', 'campaign_name', 'week', 'sa360_col_name'], columns=['source'], values='value', aggfunc='sum').reset_index()\n",
    "    df_v1['%_diff'] = abs((df_v1['redshift']/df_v1['sa360'])-1)\n",
    "    df_v1.rename(columns={'sa360_col_name':'metric'}, inplace=True)\n",
    "\n",
    "    view_args = {\n",
    "        'advertiser': {'index':['advertiser'],\n",
    "                       'dim_cutoff': 1},\n",
    "\n",
    "        'campaign': {'index':['advertiser', 'campaign_name'],\n",
    "                     'dim_cutoff': 2},\n",
    "\n",
    "        'week': {'index':['advertiser', 'campaign_name', 'week'],\n",
    "                 'dim_cutoff': 3}\n",
    "                }\n",
    "\n",
    "    # loop to calculate difference by view args keys\n",
    "    for k in view_args.keys():\n",
    "        df_pvt = df_qa_week = df_qa.pivot_table(index=view_args[k]['index'], columns=['sa360_col_name', 'source'], values='value', aggfunc='sum').reset_index()\n",
    "\n",
    "        list_of_metric_diff = []\n",
    "        metrics = df_pvt.columns.levels[0][:list(df_pvt.columns.levels[0]).index(view_args[k]['index'][-1])]\n",
    "        for m in metrics:\n",
    "            df_temp = df_pvt.copy()\n",
    "            df_temp = df_temp[m]\n",
    "            df_temp.columns = [f\"{m}_{c}\" for c in df_temp.columns]\n",
    "            df_temp[f\"{m}_%_diff\"] = (df_temp.iloc[:,1]/df_temp.iloc[:,0])-1\n",
    "            list_of_metric_diff.append(df_temp)\n",
    "\n",
    "        metric_diffs = pd.concat(list_of_metric_diff, axis=1)\n",
    "\n",
    "        df_pvt_qa = pd.concat([df_pvt.iloc[:, :view_args[k]['dim_cutoff']], metric_diffs], axis=1)\n",
    "        df_pvt_qa.columns = [c[0] if type(c) is tuple else c for c in df_pvt_qa.columns]\n",
    "\n",
    "        pvts_dict[k] = df_pvt_qa\n",
    "\n",
    "    pvts_dict['raw_data'], pvts_dict['redshift_v_sa360'] = df_v0, df_v1\n",
    "\n",
    "    return pvts_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime: 0.51 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"runtime: {np.round(time.time() - start, 2)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
