{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "\n",
    "import xlsxwriter\n",
    "from itertools import chain\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "from SharePoint import *\n",
    "from hlpr import *\n",
    "from static import *\n",
    "from format_qa import *\n",
    "from qa_by_media import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "media = 'digital'\n",
    "\n",
    "SHAREPOINT_DATA_PATH = media_args[media]['sharepoint']['data']\n",
    "SHAREPOINT_MAPPING_PATH = media_args[media]['sharepoint']['mapping']\n",
    "SHAREPOINT_QA_PATH = media_args[media]['sharepoint']['qa']\n",
    "SHAREPOINT_FLAT_PATH = media_args[media]['sharepoint']['flat']\n",
    "REDSHIFT_METRICS_QUERY = media_args[media]['redshift']['all_metrics']\n",
    "REDSHIFT_METRICS_FILENAME = media_args[media]['redshift']['metric_filename']\n",
    "REDSHIFT_METRICS_SHEETNAME = media_args[media]['redshift']['metric_sheetname']\n",
    "REDSHIFT_QA_QUERY = media_args[media]['redshift']['qa_qry']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Connect to SharePoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connected to: https://interpublic.sharepoint.com/sites/AmericanExpressUSGABM\n",
      "\n",
      "listing files from: Measurement%20%20Analytics%20Folder/GABM/Global%20Analytics/DigitalQA/Digital/01Sandbox/DCMPull/DCMReport\n",
      "\n",
      "listing files from: Measurement%20%20Analytics%20Folder/GABM/Global%20Analytics/DigitalQA/Digital/01Sandbox/SiteServed\n",
      "\n",
      "downloading data to: C:\\Users\\carmelo.urena\\Documents\\Main\\OneDrive_BAK\\Amex\\DigitalQA\\SharePoint\\sharepoint_api\\data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "app = SharePoint('AmericanExpressUSGABM')\n",
    "\n",
    "# download data\n",
    "app.list_contents(SHAREPOINT_DATA_PATH)\n",
    "# app.download_files(DATA_PATH)\n",
    "# app.archive_files()\n",
    "\n",
    "ancillary_data(app, media)\n",
    "\n",
    "# download floodlight mapping\n",
    "# app.list_contents(SHAREPOINT_MAPPING_PATH)\n",
    "# app.download_files(ASSETS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import metrics\n",
    "pth = glob.glob(os.path.join(ASSETS_PATH, f\"*{REDSHIFT_METRICS_FILENAME}*\"))[0]\n",
    "df_metrics = pd.read_excel(pth, sheet_name='ConvMap')\n",
    "\n",
    "campaign_name_list = [c for c in df_metrics['Campaign'].unique()] \n",
    "campaign_ids_list = [str(c) for c in df_metrics['CampaignID'].unique()]\n",
    "activity_list = [str(a) for a in df_metrics['ActivityID'].dropna().unique()]\n",
    "core_metrics = ['Impressions', 'Clicks', 'Media Cost', 'Video Plays', 'Video Views', 'Video Completions']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Raw Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-20ed7d983664>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;31m# combine digital data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[0mdf_ui\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_of_dfs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\carmelo.urena\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    226\u001b[0m                        \u001b[0mkeys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m                        copy=copy, sort=sort)\n\u001b[0m\u001b[0;32m    229\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\carmelo.urena\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No objects to concatenate'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "# import digital data\n",
    "list_of_dfs = []\n",
    "\n",
    "files = glob.glob(os.path.join(DATA_PATH, '*.csv'))\n",
    "for f in files:\n",
    "    df = pd.read_csv(f, skiprows=10).iloc[:-1, :]\n",
    "    df = df[df['Campaign ID'].map(str).isin(campaign_ids_list)] # filter campaigns\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    \n",
    "# core metrics\n",
    "    df_core = df[df['Activity ID']=='(not set)'].iloc[:, :-2]\n",
    "    id_vars = [c for c in df_core.columns if not(c in core_metrics)]\n",
    "    df_core = df_core.melt(id_vars=id_vars)\n",
    "    df_core['metric'] = df_core['variable'] \n",
    "    \n",
    "# activity    \n",
    "    df_act = df[df['Activity ID'].isin(activity_list)]\n",
    "    df_act.drop(columns=core_metrics, inplace=True) # remove core metrics\n",
    "\n",
    "    # create floodlight metrics\n",
    "    id_vars = df_act.columns[:-2]\n",
    "    df_act = df_act.melt(id_vars=id_vars)\n",
    "    if len(activity_list) > 0:\n",
    "        df_act['metric'] = df_act.apply(lambda x: x['Activity Group'] + ' : ' + x['Activity'] + ': ' + x['variable'], axis=1)\n",
    "    else:\n",
    "        df_act['metric'] = None # dummy\n",
    "        \n",
    "# combine core and activity\n",
    "    df_combined = pd.concat([df_core, df_act], sort=False)\n",
    "    \n",
    "    # drop activity\n",
    "    drop_activity = [c for c in df_combined.columns if 'activity' in c.lower()]\n",
    "    df_combined = df_combined.drop(columns=drop_activity)\n",
    "\n",
    "    list_of_dfs.append(df_combined)\n",
    "\n",
    "# combine digital data\n",
    "df_ui = pd.concat(list_of_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4882628', '10740700', '10728117', '10728120', '10669167']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activity_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get Parameters for DB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "db_campaign_id = \"','\".join(list(df_ui['Campaign ID'].map(str).unique()))\n",
    "db_placement_id = \"','\".join(list(df_ui['Placement ID'].map(str).unique())) \n",
    "db_start_date = datetime.strftime(df_ui['Date'].min(), '%Y-%m-%d')\n",
    "db_end_date = datetime.strftime(df_ui['Date'].max(), '%Y-%m-%d')\n",
    "db_metrics = \"','\".join(list(df_ui['metric'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DB Query**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data from redshift\n",
    "qry_txt = get_qry_text(REDSHIFT_QA_QUERY)\n",
    "qry_txt = qry_txt.replace('load_metrics', db_metrics) \\\n",
    "                 .replace('load_campaign_id', db_campaign_id) \\\n",
    "                 .replace('load_placement_id', db_placement_id) \\\n",
    "                 .replace('load_start_date', db_start_date) \\\n",
    "                 .replace('load_end_date', db_end_date)\n",
    "\n",
    "time.sleep(1)\n",
    "df_qry_raw = run_qry(qry_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qry = df_qry_raw.copy()\n",
    "df_qry_dcm = df_qry[df_qry['source']!='override'] # contains dcm & cadreon\n",
    "df_qry_ss = df_qry[df_qry['source']=='override']\n",
    "\n",
    "# df_qry_dcm.drop(columns=['source'], inplace=True)\n",
    "# df_qry_ss.drop(columns=['source'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_temp = df_qry_dcm[df_qry_dcm['campaign_name']=='USA_NAT_PRO_GA_EN_NON_GAB_21_Q1_Programmatic_Enterprise_Dine Small']\n",
    "# df_temp = df_temp[df_temp['source']=='dcm']\n",
    "# df_temp[df_temp['metric']=='Media Cost'].to_excel('dcm_pro.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_qry_ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine SS Templated to DB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_metric_rename = {\n",
    "    'clicks': 'Clicks',\n",
    "    'spend': 'Media Cost',\n",
    "    'impressions': 'Impressions',\n",
    "    'video plays': 'Video Plays',\n",
    "    'video completions': 'Video Completions'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process SS templates\n",
    "files = glob.glob(os.path.join('../data', '*xlsx'))\n",
    "\n",
    "df_ss_template = process_ss_templates(files)\n",
    "df_ss_template = df_ss_template[df_ss_template['campaign name'].isin(campaign_name_list)]\n",
    "\n",
    "# align columns to redshift\n",
    "df_ss_template['week'] = df_ss_template['date'].apply(lambda x: x - timedelta(days=x.weekday()))\n",
    "df_ss_template.rename(columns={'publisher (site)':'site_name'}, inplace=True)\n",
    "df_ss_template.rename(columns={c: c.replace(' ', '_') for c in df_ss_template.columns}, inplace=True)\n",
    "\n",
    "df_ss_template['metric'] = df_ss_template['metric'].apply(lambda x: ss_metric_rename[x])\n",
    "\n",
    "# add source\n",
    "df_ss_template['source'] = 'ui'\n",
    "df_qry_ss['source'] = 'redshift'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qa_ss = pd.concat([df_qry_ss, df_ss_template], sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qa_ss.drop(columns=['date', 'campaign_id', 'site_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qa_ss['site_served'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combined DCM UI and DB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# align ui column names\n",
    "ui_col_rename = {\n",
    "    'Date': 'week',\n",
    "    'Campaign': 'campaign_name',\n",
    "    'Campaign ID': 'campaign_id',\n",
    "    'Placement': 'placement_name',\n",
    "    'Placement ID': 'placement_id',\n",
    "    'Site (DCM)': 'site_name',\n",
    "    'Site ID (DCM)': 'site_id',\n",
    "#     'variable': 'sa360_col_name',\n",
    "    'metric': 'metric',\n",
    "    'value': 'value'\n",
    "}\n",
    "\n",
    "# filter cols in UI data\n",
    "df_ui_fltr = df_ui[ui_col_rename.keys()]\n",
    "df_ui_fltr.rename(columns=ui_col_rename, inplace=True)\n",
    "\n",
    "# combine\n",
    "df_qry_dcm['source'] = 'redshift'\n",
    "df_ui_fltr['source'] = 'ui'\n",
    "\n",
    "df_qa = pd.concat([df_qry_dcm, df_ui_fltr], sort=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data\n",
    "id_cols = [c for c in df_qa.columns if '_id' in c]\n",
    "for c in id_cols:\n",
    "    df_qa[c] = df_qa[c].map(int)\n",
    "    \n",
    "df_qa['week'] = pd.to_datetime(df_qa['week'])\n",
    "df_qa['week'] = df_qa['week'].apply(lambda x: x - timedelta(days=x.weekday()))\n",
    "\n",
    "# remove trailing spaces\n",
    "df_qa['campaign_name'] = df_qa['campaign_name'].str.rstrip()\n",
    "df_qa['placement_name'] = df_qa['placement_name'].str.rstrip()\n",
    "df_qa['site_name'] = df_qa['site_name'].str.rstrip()\n",
    "df_qa['metric'] = df_qa['metric'].str.rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qa['site_served'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine All**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qa = pd.concat([df_qa, df_qa_ss])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QA Pivots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvts_dict = dict()\n",
    "\n",
    "db_params_dict = {'last_updated':datetime.now(), 'start_date': db_start_date, 'end_date': db_end_date, 'sql_qry': qry_txt}\n",
    "details = pd.DataFrame.from_dict(db_params_dict, orient='index').reset_index()\n",
    "details.columns = ['variable', 'value']\n",
    "\n",
    "pvts_dict['details'] = details\n",
    "pvts_dict['raw_data'] = df_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ui_placements = df_qa[df_qa['source']=='dcm'][['campaign_name', 'campaign_id', 'placement_name', 'placement_id']].drop_duplicates()\n",
    "# pvts_dict['placement_details'] = ui_placements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_qa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_args = {\n",
    "    'campaign': {'index':['site_served', 'campaign_name', 'site_name'],\n",
    "                   'dim_cutoff': 3},\n",
    "        \n",
    "    'placement': {'index':['site_served', 'campaign_name', 'placement_id', 'site_name'], \n",
    "                 'dim_cutoff': 4},\n",
    "    \n",
    "    \n",
    "    'week': {'index':['site_served', 'campaign_name', 'week', 'site_name'],\n",
    "             'dim_cutoff': 4}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_qa['metric'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "digital_param_fieds = {\n",
    "    'dcm': {\n",
    "        'core_metrics': ['Clicks', 'Media Cost', 'Impressions', 'Video Plays', 'Video Views', 'Video Completions']\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in view_args.keys():\n",
    "    df_pvt =  df_qa.pivot_table(index=view_args[k]['index'], columns=['metric', 'source'], values='value', aggfunc='sum').reset_index()\n",
    "    \n",
    "    list_of_metric_diff = []\n",
    "    metrics = df_pvt.columns.levels[0][:list(df_pvt.columns.levels[0]).index(view_args[k]['index'][-1])]\n",
    "    \n",
    "    # move core metrics to the beginning\n",
    "    core_metrics = digital_param_fieds['dcm']['core_metrics']\n",
    "    metrics = core_metrics + [m for m in metrics if not(m in core_metrics)]\n",
    "    \n",
    "    for m in metrics:\n",
    "        df_temp = df_pvt.copy()\n",
    "        df_temp = df_temp[m]\n",
    "        df_temp.columns = [f\"{m}_{c}\" for c in df_temp.columns]\n",
    "        \n",
    "        # fill na based on condition\n",
    "        if len(df_temp.columns) < 2:\n",
    "            continue\n",
    "        col0 = df_temp.columns[0]\n",
    "        col1 = df_temp.columns[1]\n",
    "        df_temp[col0] = df_temp.apply(lambda x: 0 if np.isnan(x[col0]) and not(np.isnan(x[col1])) else x[col0], axis=1)\n",
    "        df_temp[col1] = df_temp.apply(lambda x: 0 if np.isnan(x[col1]) and not(np.isnan(x[col0])) else x[col1], axis=1)\n",
    "        \n",
    "        df_temp[f\"{m}_%_diff\"] = (df_temp.iloc[:,1]/df_temp.iloc[:,0])-1\n",
    "        df_temp[f\"{m}_%_diff\"] = df_temp[f\"{m}_%_diff\"].apply(lambda x: 1 if x == float('inf') else x)\n",
    "        \n",
    "        list_of_metric_diff.append(df_temp)\n",
    "        \n",
    "    metric_diffs = pd.concat(list_of_metric_diff, axis=1)\n",
    "    df_pvt_qa = pd.concat([df_pvt.iloc[:, :view_args[k]['dim_cutoff']], metric_diffs], axis=1)\n",
    "    df_pvt_qa.columns = [c[0] if type(c) is tuple else c for c in df_pvt_qa.columns]\n",
    "    \n",
    "    pvts_dict[k] = df_pvt_qa\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['details', 'raw_data', 'campaign', 'placement', 'week'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pvts_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Export**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_filename = f\"QA_Digital_{app.dt}.xlsx\"\n",
    "writer = pd.ExcelWriter(os.path.join(OUTPUTS_PATH, qa_filename), engine='xlsxwriter')\n",
    "\n",
    "for k in pvts_dict.keys():\n",
    "    pvts_dict[k].to_excel(writer, sheet_name=f\"{k}\", index=False)\n",
    "        \n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# media = 'digital'\n",
    "# qa_filename = 'QA_Digital_20210224.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_qa(media, os.path.join(OUTPUTS_PATH, qa_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Archiver**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def archive_data(medias):\n",
    "    app = SharePoint('AmericanExpressUSGABM')\n",
    "    \n",
    "    print(f\"\\n archiving data\\n{'='*50}\")\n",
    "          \n",
    "    for media in medias:\n",
    "        SHAREPOINT_DATA_PATH = media_args[media]['sharepoint']['data']\n",
    "        app.list_contents(SHAREPOINT_DATA_PATH)\n",
    "        app.download_files(DATA_PATH)\n",
    "        app.archive_files()\n",
    "        \n",
    "        if media == 'digital':\n",
    "            app.list_contents(media_args[media]['sharepoint']['ancillary_data'])\n",
    "            app.download_files(DATA_PATH)\n",
    "            app.archive_files()\n",
    "          \n",
    "    delete_local_data(False)\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "medias = ['search', 'social', 'digital']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connected to: https://interpublic.sharepoint.com/sites/AmericanExpressUSGABM\n",
      "\n",
      "\n",
      " archiving data\n",
      "==================================================\n",
      "listing files from: Measurement%20%20Analytics%20Folder/GABM/Global%20Analytics/DigitalQA/Search/01Sandbox/SA360_Files/Gmail/National\n",
      "\n",
      "downloading data to: C:\\Users\\carmelo.urena\\Documents\\Main\\OneDrive_BAK\\Amex\\DigitalQA\\SharePoint\\sharepoint_api\\data\n",
      "\n",
      "archiving data in SharePoint\n",
      "\n",
      "listing files from: Measurement%20%20Analytics%20Folder/GABM/Global%20Analytics/DigitalQA/Social/01Sandbox/National\n",
      "\n",
      "downloading data to: C:\\Users\\carmelo.urena\\Documents\\Main\\OneDrive_BAK\\Amex\\DigitalQA\\SharePoint\\sharepoint_api\\data\n",
      "\n",
      "archiving data in SharePoint\n",
      "\n",
      "listing files from: Measurement%20%20Analytics%20Folder/GABM/Global%20Analytics/DigitalQA/Digital/01Sandbox/DCMPull/DCMReport\n",
      "\n",
      "downloading data to: C:\\Users\\carmelo.urena\\Documents\\Main\\OneDrive_BAK\\Amex\\DigitalQA\\SharePoint\\sharepoint_api\\data\n",
      "\n",
      "archiving data in SharePoint\n",
      "\n",
      "listing files from: Measurement%20%20Analytics%20Folder/GABM/Global%20Analytics/DigitalQA/Digital/01Sandbox/SiteServed\n",
      "\n",
      "downloading data to: C:\\Users\\carmelo.urena\\Documents\\Main\\OneDrive_BAK\\Amex\\DigitalQA\\SharePoint\\sharepoint_api\\data\n",
      "\n",
      "archiving data in SharePoint\n",
      "\n",
      "\n",
      "deleting data in data folder\n"
     ]
    }
   ],
   "source": [
    "archive_data(medias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log = pd.read_csv(os.path.join(OUTPUTS_PATH, 'log.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
